
nameOverride: "kafka-to-s3"
fullnameOverride: "kafka-to-s3"

env:
  - name: ACTION
    value: 'dump'
  #
  - name: DUMP_BROKER
    value: 'my-cluster-kafka-brokers:9092'
  - name: DUMP_TOPIC
    value: 'my-topic'
  - name: DUMP_PARTITION
    value: '0'
  #
  - name: S3_REGION
    value: 'eu-central-1'
  - name: S3_BUCKET
    value: 'kafka-backup'
  - name: S3_PATH
    value: 'my-cluster/topics'
  #
  - name: AWS_ACCESS_KEY_ID
    valueFrom:
      secretKeyRef:
        name: s3-kafka-backup
        key: AWS_ACCESS_KEY_ID
  - name: AWS_SECRET_ACCESS_KEY
    valueFrom:
      secretKeyRef:
        name: s3-kafka-backup
        key: AWS_SECRET_ACCESS_KEY
  #
  - name: DUMP_FLUSH_SIZE
    value: '100_000'  # batch size to wait before sending to s3
  - name: DUMP_FLUSH_PERIOD_MS
    value: '60_000'  # wait before sending to s3 even if batch is small
  - name: DUMP_CONSUMER_TIMEOUT_MS
    value: '30_000'  # send the batch even if there is no messages this long
  #

image:
  repository: python
  tag: "3"
  pullPolicy: IfNotPresent

command:
  - bash
  - -c
  - |
    pip install kafka-python boto3
    exec /scripts/k2s3.py

resources:
  requests:
    memory: "500Mi"
    cpu: "10m"
  limits:
    memory: "500Mi"
    cpu: "500m"
securityContext: {}
startupProbe: {}
livenessProbe: {}
readinessProbe: {}
ports: []
nodeSelector: {}
tolerations: []
affinity: {}



